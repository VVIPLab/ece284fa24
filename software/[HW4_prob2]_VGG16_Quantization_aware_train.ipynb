{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "ResNet_Cifar(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(\n",
      "          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (weight_quant): weight_quantize_fn()\n",
      "        )\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(\n",
      "          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (weight_quant): weight_quantize_fn()\n",
      "        )\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "     \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"VGG16_quant\"\n",
    "model = VGG16_quant()\n",
    "\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [150, 225]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "junior-reminder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 0.681 (0.681)\tData 0.550 (0.550)\tLoss 2.4067 (2.4067)\tPrec 11.719% (11.719%)\n",
      "Epoch: [0][100/391]\tTime 0.050 (0.060)\tData 0.002 (0.008)\tLoss 1.7840 (1.9610)\tPrec 31.250% (25.046%)\n",
      "Epoch: [0][200/391]\tTime 0.059 (0.058)\tData 0.003 (0.005)\tLoss 1.5728 (1.8040)\tPrec 41.406% (31.705%)\n",
      "Epoch: [0][300/391]\tTime 0.038 (0.055)\tData 0.002 (0.004)\tLoss 1.6187 (1.7079)\tPrec 36.719% (35.810%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.653 (0.653)\tLoss 1.4142 (1.4142)\tPrec 46.875% (46.875%)\n",
      " * Prec 48.190% \n",
      "best acc: 48.190000\n",
      "Epoch: [1][0/391]\tTime 0.668 (0.668)\tData 0.589 (0.589)\tLoss 1.4352 (1.4352)\tPrec 45.312% (45.312%)\n",
      "Epoch: [1][100/391]\tTime 0.056 (0.059)\tData 0.002 (0.008)\tLoss 1.1964 (1.2429)\tPrec 57.031% (54.819%)\n",
      "Epoch: [1][200/391]\tTime 0.058 (0.055)\tData 0.002 (0.005)\tLoss 1.1786 (1.2040)\tPrec 57.031% (56.219%)\n",
      "Epoch: [1][300/391]\tTime 0.067 (0.055)\tData 0.003 (0.004)\tLoss 1.2200 (1.1755)\tPrec 59.375% (57.350%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.457 (0.457)\tLoss 1.1117 (1.1117)\tPrec 56.250% (56.250%)\n",
      " * Prec 60.510% \n",
      "best acc: 60.510000\n",
      "Epoch: [2][0/391]\tTime 0.754 (0.754)\tData 0.679 (0.679)\tLoss 1.0652 (1.0652)\tPrec 62.500% (62.500%)\n",
      "Epoch: [2][100/391]\tTime 0.039 (0.060)\tData 0.002 (0.009)\tLoss 0.9460 (0.9868)\tPrec 67.969% (64.302%)\n",
      "Epoch: [2][200/391]\tTime 0.064 (0.053)\tData 0.003 (0.006)\tLoss 0.9682 (0.9671)\tPrec 64.062% (65.232%)\n",
      "Epoch: [2][300/391]\tTime 0.062 (0.054)\tData 0.003 (0.005)\tLoss 1.0186 (0.9535)\tPrec 65.625% (65.809%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.450 (0.450)\tLoss 1.2695 (1.2695)\tPrec 56.250% (56.250%)\n",
      " * Prec 60.740% \n",
      "best acc: 60.740000\n",
      "Epoch: [3][0/391]\tTime 0.601 (0.601)\tData 0.521 (0.521)\tLoss 0.9612 (0.9612)\tPrec 70.312% (70.312%)\n",
      "Epoch: [3][100/391]\tTime 0.059 (0.059)\tData 0.002 (0.008)\tLoss 0.8678 (0.8472)\tPrec 71.875% (69.794%)\n",
      "Epoch: [3][200/391]\tTime 0.052 (0.057)\tData 0.002 (0.005)\tLoss 0.7433 (0.8330)\tPrec 70.312% (70.476%)\n",
      "Epoch: [3][300/391]\tTime 0.054 (0.055)\tData 0.002 (0.004)\tLoss 0.8927 (0.8214)\tPrec 64.844% (70.998%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.649 (0.649)\tLoss 0.6589 (0.6589)\tPrec 75.781% (75.781%)\n",
      " * Prec 73.480% \n",
      "best acc: 73.480000\n",
      "Epoch: [4][0/391]\tTime 1.097 (1.097)\tData 1.019 (1.019)\tLoss 0.6512 (0.6512)\tPrec 75.000% (75.000%)\n",
      "Epoch: [4][100/391]\tTime 0.050 (0.065)\tData 0.003 (0.013)\tLoss 0.8609 (0.7465)\tPrec 71.094% (73.685%)\n",
      "Epoch: [4][200/391]\tTime 0.048 (0.060)\tData 0.003 (0.008)\tLoss 0.7467 (0.7482)\tPrec 73.438% (73.908%)\n",
      "Epoch: [4][300/391]\tTime 0.040 (0.056)\tData 0.002 (0.006)\tLoss 0.7100 (0.7383)\tPrec 76.562% (74.227%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.956 (0.956)\tLoss 0.6974 (0.6974)\tPrec 75.000% (75.000%)\n",
      " * Prec 72.590% \n",
      "best acc: 73.480000\n",
      "Epoch: [5][0/391]\tTime 0.881 (0.881)\tData 0.805 (0.805)\tLoss 0.5666 (0.5666)\tPrec 82.031% (82.031%)\n",
      "Epoch: [5][100/391]\tTime 0.049 (0.061)\tData 0.003 (0.010)\tLoss 0.7363 (0.6861)\tPrec 77.344% (75.619%)\n",
      "Epoch: [5][200/391]\tTime 0.064 (0.057)\tData 0.002 (0.006)\tLoss 0.6326 (0.6800)\tPrec 75.000% (76.065%)\n",
      "Epoch: [5][300/391]\tTime 0.060 (0.056)\tData 0.003 (0.005)\tLoss 0.5474 (0.6731)\tPrec 78.125% (76.503%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.605 (0.605)\tLoss 0.6980 (0.6980)\tPrec 72.656% (72.656%)\n",
      " * Prec 74.790% \n",
      "best acc: 74.790000\n",
      "Epoch: [6][0/391]\tTime 0.762 (0.762)\tData 0.683 (0.683)\tLoss 0.5976 (0.5976)\tPrec 78.125% (78.125%)\n",
      "Epoch: [6][100/391]\tTime 0.045 (0.059)\tData 0.002 (0.009)\tLoss 0.7103 (0.6133)\tPrec 74.219% (78.458%)\n",
      "Epoch: [6][200/391]\tTime 0.060 (0.056)\tData 0.003 (0.006)\tLoss 0.6256 (0.6275)\tPrec 77.344% (77.919%)\n",
      "Epoch: [6][300/391]\tTime 0.052 (0.054)\tData 0.002 (0.005)\tLoss 0.6102 (0.6253)\tPrec 76.562% (78.039%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.803 (0.803)\tLoss 0.6593 (0.6593)\tPrec 75.000% (75.000%)\n",
      " * Prec 71.840% \n",
      "best acc: 74.790000\n",
      "Epoch: [7][0/391]\tTime 0.615 (0.615)\tData 0.542 (0.542)\tLoss 0.5560 (0.5560)\tPrec 81.250% (81.250%)\n",
      "Epoch: [7][100/391]\tTime 0.042 (0.060)\tData 0.002 (0.008)\tLoss 0.7747 (0.5966)\tPrec 71.875% (79.115%)\n",
      "Epoch: [7][200/391]\tTime 0.038 (0.053)\tData 0.002 (0.005)\tLoss 0.5554 (0.5889)\tPrec 81.250% (79.555%)\n",
      "Epoch: [7][300/391]\tTime 0.061 (0.051)\tData 0.003 (0.004)\tLoss 0.5610 (0.5843)\tPrec 81.250% (79.724%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.759 (0.759)\tLoss 0.5801 (0.5801)\tPrec 78.125% (78.125%)\n",
      " * Prec 76.440% \n",
      "best acc: 76.440000\n",
      "Epoch: [8][0/391]\tTime 0.615 (0.615)\tData 0.535 (0.535)\tLoss 0.4105 (0.4105)\tPrec 85.156% (85.156%)\n",
      "Epoch: [8][100/391]\tTime 0.052 (0.049)\tData 0.002 (0.007)\tLoss 0.7590 (0.5537)\tPrec 72.656% (81.080%)\n",
      "Epoch: [8][200/391]\tTime 0.046 (0.049)\tData 0.002 (0.005)\tLoss 0.5387 (0.5620)\tPrec 82.812% (80.640%)\n",
      "Epoch: [8][300/391]\tTime 0.074 (0.050)\tData 0.003 (0.004)\tLoss 0.4957 (0.5546)\tPrec 82.812% (80.788%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.658 (0.658)\tLoss 0.5237 (0.5237)\tPrec 84.375% (84.375%)\n",
      " * Prec 78.480% \n",
      "best acc: 78.480000\n",
      "Epoch: [9][0/391]\tTime 0.821 (0.821)\tData 0.763 (0.763)\tLoss 0.3766 (0.3766)\tPrec 87.500% (87.500%)\n",
      "Epoch: [9][100/391]\tTime 0.049 (0.062)\tData 0.002 (0.010)\tLoss 0.6040 (0.5272)\tPrec 77.344% (81.590%)\n",
      "Epoch: [9][200/391]\tTime 0.054 (0.057)\tData 0.002 (0.006)\tLoss 0.6443 (0.5276)\tPrec 77.344% (81.736%)\n",
      "Epoch: [9][300/391]\tTime 0.036 (0.054)\tData 0.002 (0.005)\tLoss 0.5742 (0.5261)\tPrec 82.812% (81.896%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.491 (0.491)\tLoss 0.4073 (0.4073)\tPrec 84.375% (84.375%)\n",
      " * Prec 79.730% \n",
      "best acc: 79.730000\n",
      "Epoch: [10][0/391]\tTime 0.783 (0.783)\tData 0.700 (0.700)\tLoss 0.4584 (0.4584)\tPrec 82.812% (82.812%)\n",
      "Epoch: [10][100/391]\tTime 0.045 (0.059)\tData 0.002 (0.009)\tLoss 0.5165 (0.5011)\tPrec 84.375% (82.379%)\n",
      "Epoch: [10][200/391]\tTime 0.052 (0.052)\tData 0.002 (0.006)\tLoss 0.5361 (0.5042)\tPrec 80.469% (82.455%)\n",
      "Epoch: [10][300/391]\tTime 0.050 (0.050)\tData 0.002 (0.005)\tLoss 0.5062 (0.5058)\tPrec 81.250% (82.408%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.657 (0.657)\tLoss 0.4073 (0.4073)\tPrec 87.500% (87.500%)\n",
      " * Prec 80.750% \n",
      "best acc: 80.750000\n",
      "Epoch: [11][0/391]\tTime 0.620 (0.620)\tData 0.547 (0.547)\tLoss 0.5024 (0.5024)\tPrec 82.031% (82.031%)\n",
      "Epoch: [11][100/391]\tTime 0.053 (0.058)\tData 0.002 (0.008)\tLoss 0.5585 (0.4680)\tPrec 75.000% (84.089%)\n",
      "Epoch: [11][200/391]\tTime 0.058 (0.058)\tData 0.003 (0.005)\tLoss 0.6211 (0.4767)\tPrec 78.906% (83.532%)\n",
      "Epoch: [11][300/391]\tTime 0.040 (0.058)\tData 0.002 (0.004)\tLoss 0.5099 (0.4818)\tPrec 85.938% (83.355%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.619 (0.619)\tLoss 0.5546 (0.5546)\tPrec 82.812% (82.812%)\n",
      " * Prec 77.810% \n",
      "best acc: 80.750000\n",
      "Epoch: [12][0/391]\tTime 0.705 (0.705)\tData 0.628 (0.628)\tLoss 0.5161 (0.5161)\tPrec 86.719% (86.719%)\n",
      "Epoch: [12][100/391]\tTime 0.054 (0.062)\tData 0.003 (0.009)\tLoss 0.4892 (0.4556)\tPrec 81.250% (84.050%)\n",
      "Epoch: [12][200/391]\tTime 0.061 (0.060)\tData 0.003 (0.006)\tLoss 0.4071 (0.4629)\tPrec 86.719% (83.924%)\n",
      "Epoch: [12][300/391]\tTime 0.052 (0.059)\tData 0.002 (0.005)\tLoss 0.3528 (0.4614)\tPrec 85.156% (83.962%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.356 (0.356)\tLoss 0.3954 (0.3954)\tPrec 84.375% (84.375%)\n",
      " * Prec 81.230% \n",
      "best acc: 81.230000\n",
      "Epoch: [13][0/391]\tTime 0.735 (0.735)\tData 0.659 (0.659)\tLoss 0.4083 (0.4083)\tPrec 85.156% (85.156%)\n",
      "Epoch: [13][100/391]\tTime 0.038 (0.056)\tData 0.002 (0.009)\tLoss 0.4758 (0.4447)\tPrec 85.938% (84.754%)\n",
      "Epoch: [13][200/391]\tTime 0.058 (0.052)\tData 0.002 (0.006)\tLoss 0.5403 (0.4459)\tPrec 81.250% (84.523%)\n",
      "Epoch: [13][300/391]\tTime 0.040 (0.051)\tData 0.002 (0.004)\tLoss 0.3652 (0.4410)\tPrec 85.938% (84.741%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.551 (0.551)\tLoss 0.4791 (0.4791)\tPrec 79.688% (79.688%)\n",
      " * Prec 80.660% \n",
      "best acc: 81.230000\n",
      "Epoch: [14][0/391]\tTime 0.719 (0.719)\tData 0.641 (0.641)\tLoss 0.4623 (0.4623)\tPrec 85.938% (85.938%)\n",
      "Epoch: [14][100/391]\tTime 0.049 (0.057)\tData 0.002 (0.009)\tLoss 0.4114 (0.4369)\tPrec 86.719% (85.195%)\n",
      "Epoch: [14][200/391]\tTime 0.061 (0.058)\tData 0.003 (0.006)\tLoss 0.3410 (0.4309)\tPrec 85.938% (85.117%)\n",
      "Epoch: [14][300/391]\tTime 0.059 (0.058)\tData 0.003 (0.005)\tLoss 0.4351 (0.4300)\tPrec 82.031% (85.104%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.612 (0.612)\tLoss 0.3879 (0.3879)\tPrec 83.594% (83.594%)\n",
      " * Prec 81.330% \n",
      "best acc: 81.330000\n",
      "Epoch: [15][0/391]\tTime 0.791 (0.791)\tData 0.714 (0.714)\tLoss 0.4967 (0.4967)\tPrec 83.594% (83.594%)\n",
      "Epoch: [15][100/391]\tTime 0.061 (0.060)\tData 0.003 (0.009)\tLoss 0.4186 (0.4146)\tPrec 85.156% (85.566%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [15][200/391]\tTime 0.058 (0.053)\tData 0.003 (0.006)\tLoss 0.4047 (0.4204)\tPrec 87.500% (85.463%)\n",
      "Epoch: [15][300/391]\tTime 0.058 (0.055)\tData 0.003 (0.005)\tLoss 0.3754 (0.4164)\tPrec 88.281% (85.574%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.626 (0.626)\tLoss 0.3551 (0.3551)\tPrec 87.500% (87.500%)\n",
      " * Prec 84.260% \n",
      "best acc: 84.260000\n",
      "Epoch: [16][0/391]\tTime 0.683 (0.683)\tData 0.611 (0.611)\tLoss 0.3852 (0.3852)\tPrec 85.156% (85.156%)\n",
      "Epoch: [16][100/391]\tTime 0.046 (0.055)\tData 0.003 (0.008)\tLoss 0.3029 (0.3897)\tPrec 89.844% (86.061%)\n",
      "Epoch: [16][200/391]\tTime 0.054 (0.053)\tData 0.002 (0.005)\tLoss 0.4763 (0.3954)\tPrec 84.375% (85.992%)\n",
      "Epoch: [16][300/391]\tTime 0.062 (0.052)\tData 0.003 (0.004)\tLoss 0.4451 (0.3982)\tPrec 86.719% (86.023%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.449 (0.449)\tLoss 0.4246 (0.4246)\tPrec 85.156% (85.156%)\n",
      " * Prec 82.080% \n",
      "best acc: 84.260000\n",
      "Epoch: [17][0/391]\tTime 0.790 (0.790)\tData 0.706 (0.706)\tLoss 0.4263 (0.4263)\tPrec 79.688% (79.688%)\n",
      "Epoch: [17][100/391]\tTime 0.056 (0.059)\tData 0.003 (0.009)\tLoss 0.3365 (0.3972)\tPrec 92.188% (86.332%)\n",
      "Epoch: [17][200/391]\tTime 0.053 (0.052)\tData 0.002 (0.006)\tLoss 0.3196 (0.3952)\tPrec 90.625% (86.400%)\n",
      "Epoch: [17][300/391]\tTime 0.039 (0.051)\tData 0.002 (0.005)\tLoss 0.3546 (0.3937)\tPrec 85.938% (86.503%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.519 (0.519)\tLoss 0.5843 (0.5843)\tPrec 83.594% (83.594%)\n",
      " * Prec 79.200% \n",
      "best acc: 84.260000\n",
      "Epoch: [18][0/391]\tTime 0.818 (0.818)\tData 0.743 (0.743)\tLoss 0.4169 (0.4169)\tPrec 85.156% (85.156%)\n",
      "Epoch: [18][100/391]\tTime 0.046 (0.058)\tData 0.002 (0.010)\tLoss 0.2857 (0.3657)\tPrec 89.844% (87.508%)\n",
      "Epoch: [18][200/391]\tTime 0.044 (0.054)\tData 0.002 (0.006)\tLoss 0.3294 (0.3795)\tPrec 87.500% (86.847%)\n",
      "Epoch: [18][300/391]\tTime 0.043 (0.052)\tData 0.002 (0.005)\tLoss 0.5418 (0.3812)\tPrec 85.156% (86.742%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.639 (0.639)\tLoss 0.3919 (0.3919)\tPrec 89.062% (89.062%)\n",
      " * Prec 81.930% \n",
      "best acc: 84.260000\n",
      "Epoch: [19][0/391]\tTime 0.700 (0.700)\tData 0.638 (0.638)\tLoss 0.5562 (0.5562)\tPrec 77.344% (77.344%)\n",
      "Epoch: [19][100/391]\tTime 0.043 (0.054)\tData 0.002 (0.009)\tLoss 0.4434 (0.3788)\tPrec 87.500% (86.974%)\n",
      "Epoch: [19][200/391]\tTime 0.036 (0.052)\tData 0.002 (0.005)\tLoss 0.3895 (0.3739)\tPrec 85.938% (87.216%)\n",
      "Epoch: [19][300/391]\tTime 0.060 (0.050)\tData 0.003 (0.004)\tLoss 0.2865 (0.3768)\tPrec 89.062% (87.033%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.509 (0.509)\tLoss 0.4285 (0.4285)\tPrec 84.375% (84.375%)\n",
      " * Prec 81.850% \n",
      "best acc: 84.260000\n"
     ]
    }
   ],
   "source": [
    "# This cell won't be given, but students will complete the training\n",
    "\n",
    "lr = 4e-2\n",
    "weight_decay = 1e-4\n",
    "epochs = 20\n",
    "best_prec = 0\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "fdir = 'result/'+str(model_name)\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "        \n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HW\n",
    "\n",
    "#  1. Train with 4 bits for both weight and activation to achieve >90% accuracy\n",
    "#  2. Find x_int and w_int for the 2nd convolution layer\n",
    "#  3. Check the recovered psum has similar value to the un-quantized original psum\n",
    "#     (such as example 1 in W3S2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"result/VGG16_quant/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "#send an input and grap the value by using prehook like HW3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_bit = 4\n",
    "weight_q = model.features[3].weight_q # quantized value is stored during the training\n",
    "w_alpha = ?   # alpha is defined in your model already. bring it out here\n",
    "w_delta = ?   # delta can be calculated by using alpha and w_bit\n",
    "weight_int = ? # w_int can be calculated by weight_q and w_delta\n",
    "print(weight_int) # you should see clean integer numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-oxygen",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_bit = 4    \n",
    "x = ?  # input of the 2nd conv layer\n",
    "x_alpha  = ?\n",
    "x_delta = ??\n",
    "\n",
    "act_quant_fn = act_quantization(x_bit) # define the quantization function\n",
    "x_q = act_quant_fn(x, x_alpha)         # create the quantized value for x\n",
    "\n",
    "x_int = ?\n",
    "print(x_int) # you should see clean integer numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_int = torch.nn.Conv2d(in_channels = 64, out_channels=64, kernel_size = 3, bias = False)\n",
    "conv_int.weight = torch.nn.parameter.Parameter(weight_int)\n",
    "\n",
    "output_int =  ??    # output_int can be calculated with conv_int and x_int\n",
    "output_recovered = ??  # recover with x_delta and w_delta\n",
    "print(output_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### input floating number / weight quantized version\n",
    "\n",
    "conv_ref = torch.nn.Conv2d(in_channels = 64, out_channels=64, kernel_size = 3, bias = False)\n",
    "conv_ref.weight = model.features[3].weight_q \n",
    "\n",
    "output_ref = conv_ref(x)\n",
    "print(output_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157dffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = abs( output_ref - output_recovered )\n",
    "print(difference.mean())  ## It should be small, e.g.,2.3 in my trainned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### input floating number / weight floating number version\n",
    "\n",
    "conv_ref = torch.nn.Conv2d(in_channels = 64, out_channels=64, kernel_size = 3, bias = False)\n",
    "weight = model.features[3].weight\n",
    "mean = weight.data.mean()\n",
    "std = weight.data.std()\n",
    "conv_ref.weight = torch.nn.parameter.Parameter(weight.add(-mean).div(std))\n",
    "\n",
    "output_ref = conv_ref(x)\n",
    "print(output_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = abs( output_ref - output_recovered )\n",
    "print(difference.mean())  ## It should be small, e.g.,2.3 in my trainned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-significance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-witch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-barbados",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-serbia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
