{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "miniature-siemens",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def weight_quantization(b):\n",
    "\n",
    "    def uniform_quant(x, b):\n",
    "        xdiv = x.mul((2 ** b - 1))\n",
    "        xhard = xdiv.round().div(2 ** b - 1)  \n",
    "        #print('uniform quant bit: ', b)\n",
    "        return xhard\n",
    "\n",
    "    class quant(torch.autograd.Function):      \n",
    "        @staticmethod\n",
    "        def forward(ctx, input, alpha):\n",
    "            input.div_(alpha)                          # weights are first divided by alpha\n",
    "            input_c = input.clamp(min=-1, max=1)       # then clipped to [-1,1]\n",
    "            sign = input_c.sign()\n",
    "            input_abs = input_c.abs()\n",
    "            input_q = uniform_quant(input_abs, b).mul(sign)\n",
    "            ctx.save_for_backward(input, input_q)\n",
    "            input_q = input_q.mul(alpha)               # rescale to the original range\n",
    "            return input_q\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            grad_input = grad_output.clone()             # grad for weights will not be clipped\n",
    "            input, input_q = ctx.saved_tensors\n",
    "            i = (input.abs()>1.).float()     # >1 means clipped regime \n",
    "            sign = input.sign()              # output matrix is a form of [+1, -1, -1, +1, ...]\n",
    "            grad_alpha = (grad_output*sign*i).sum()\n",
    "            grad_input = grad_input*(1-i)\n",
    "            # above line, if i = True,  and sign = +1, \"grad_alpha = grad_output * 1\"\n",
    "            \n",
    "            return grad_input, grad_alpha    # Because we have two inputs, outputs two gradients.\n",
    "\n",
    "    return quant().apply  # such as \"relu = MyReLU().apply\" in the above cell, the function quant itself is passed.\n",
    "                        # to see, please print \"MyReLU().apply\" in the above cell\n",
    "\n",
    "\n",
    "\n",
    "class weight_quantize_fn(nn.Module):\n",
    "    def __init__(self, w_bit):\n",
    "        super(weight_quantize_fn, self).__init__()\n",
    "        self.w_bit = w_bit-1\n",
    "        self.weight_q = weight_quantization(b=self.w_bit)\n",
    "        self.wgt_alpha = torch.nn.Parameter(torch.tensor(3.0))\n",
    "\n",
    "    def forward(self, weight):\n",
    "        #mean = weight.data.mean()  # normalization provides better quantization accuracy\n",
    "        #std = weight.data.std()\n",
    "        #weight = weight.add(-mean).div(std)      # weights normalization\n",
    "        weight_q = self.weight_q(weight, self.wgt_alpha)\n",
    "        \n",
    "        return weight_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "functional-booking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.6000, -1.0000,  2.4000], grad_fn=<quantBackward>)\n"
     ]
    }
   ],
   "source": [
    "weight_quant_fn = weight_quantize_fn(w_bit=5)\n",
    "a  = torch.tensor([1.5,-1.,2.5], requires_grad=True)\n",
    "#a  = torch.tensor([1.5,-1.,3.5], requires_grad=True)\n",
    "\n",
    "a_q = weight_quant_fn(a)\n",
    "print(a_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fleet-packet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor(3., requires_grad=True)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(weight_quant_fn.wgt_alpha)\n",
    "print(weight_quant_fn.wgt_alpha.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prostate-survivor",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a_q.sum()\n",
    "c.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "anticipated-light",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(weight_quant_fn.wgt_alpha.grad)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f328fec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c710f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
